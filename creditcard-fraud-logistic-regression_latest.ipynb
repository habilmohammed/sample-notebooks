{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mIntialized\u001b[92m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sklearn\n",
    "import numpy as np \n",
    "#import mlflow\n",
    "\n",
    "from neustar import onetru\n",
    "#from neustar import ipaas\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"gs://nb-datasets/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #Dataset details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of input variables from target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.iloc[:, 1:30].columns\n",
    "target = df.iloc[:1, 30:].columns\n",
    "\n",
    "data_features = df[feature_names]\n",
    "data_target = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, data_target, \n",
    "                                                    train_size = 0.70, test_size = 0.30, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Regression Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train) #lx:Logistic_Regression Algo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "\n",
    "import pickle\n",
    "filename = 'lr_model.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric: Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - Model performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintStats(cmat, y_test, pred):\n",
    "    tpos = cmat[0][0]\n",
    "    fneg = cmat[1][1]\n",
    "    fpos = cmat[0][1]\n",
    "    tneg = cmat[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    pred = model.predict(X_test)\n",
    "    matrix = confusion_matrix(y_test, pred)\n",
    "    return matrix, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report - Model performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat, pred = RunModel(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991573329588147"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85308\n",
      "           1       0.83      0.59      0.69       135\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.92      0.79      0.84     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rmse, mae) = eval_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model from sklearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'demo-7'.\n",
      "2022/11/16 10:38:41 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: demo-7, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInfo(artifact_path='demo-7', flavors={'python_function': {'model_path': 'model.pkl', 'loader_module': 'mlflow.sklearn', 'python_version': '3.7.12', 'env': 'conda.yaml'}, 'sklearn': {'pickled_model': 'model.pkl', 'sklearn_version': '1.0.2', 'serialization_format': 'cloudpickle', 'code': None}}, model_uri='runs:/5e19ad28bf4944a5805543e3d933b813/demo-7', model_uuid='ef4354df251c40fab7b7b066a0dfe249', run_id='5e19ad28bf4944a5805543e3d933b813', saved_input_example_info=None, signature_dict=None, utc_time_created='2022-11-16 10:38:38.069116', mlflow_version='1.28.1.dev0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'demo-7'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run created successfully'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onetru.analytics.register_model('demo-7',lr,'scikit_learn',{},{\"rmse\":rmse,\"mae\":mae,\"Accuracy_score\":ac})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function \"len\" counts the number of classes = 1 and saves it as an object \"fraud_records\"\n",
    "fraud_records = len(df[df.Class == 1])\n",
    "\n",
    "# Defines the index for fraud and non-fraud in the lines:\n",
    "fraud_indices = df[df.Class == 1].index\n",
    "not_fraud_indices = df[df.Class == 0].index\n",
    "\n",
    "# Randomly collect equal samples of each type:\n",
    "under_sample_indices = np.random.choice(not_fraud_indices, fraud_records, False)\n",
    "df_undersampled = df.iloc[np.concatenate([fraud_indices, under_sample_indices]),:]\n",
    "X_undersampled = df_undersampled.iloc[:,1:30]\n",
    "Y_undersampled = df_undersampled.Class\n",
    "X_undersampled_train, X_undersampled_test, Y_undersampled_train, Y_undersampled_test = train_test_split(X_undersampled, Y_undersampled, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the \"new\" classifier for balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_undersampled = LogisticRegression()\n",
    "cmat, pred = RunModel(lr_undersampled, X_undersampled_train, Y_undersampled_train, X_undersampled_test, Y_undersampled_test)\n",
    "PrintStats(cmat, Y_undersampled_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358108108108109"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_undersampled_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       143\n",
      "           1       0.95      0.92      0.94       153\n",
      "\n",
      "    accuracy                           0.94       296\n",
      "   macro avg       0.94      0.94      0.94       296\n",
      "weighted avg       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(Y_undersampled_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_undersampled = LogisticRegression()\n",
    "cmat, pred = RunModel(lr_undersampled, X_undersampled_train, Y_undersampled_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     85308\n",
      "           1       0.04      0.91      0.08       135\n",
      "\n",
      "    accuracy                           0.97     85443\n",
      "   macro avg       0.52      0.94      0.53     85443\n",
      "weighted avg       1.00      0.97      0.98     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Parameter optimization || Retrieve logged metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 1, 'penalty': 'l2'}, 0.9126964295629962)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"C\": [1,2,3,4,5,6,7,8,9,10], \n",
    "              \"penalty\": ['l1','l2']} #Parameters\n",
    "            \n",
    "grid_search = GridSearchCV(lr, param_grid, scoring=\"precision\") #score\n",
    "grid_search.fit(y_test, pred)\n",
    "\n",
    "lr = grid_search.best_estimator_ \n",
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new parameters (Better score)\n",
    "grid = {\n",
    "    'C': [1e-4, 1e-3, 1e-2],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1e4, 1e5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.0001, 0.001, 0.01],\n",
       "                         'max_iter': [10000.0, 100000.0], 'solver': ['lbfgs']})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train a model\n",
    "\n",
    "model = linear_model.LogisticRegression(multi_class='auto')\n",
    "grid_search = model_selection.GridSearchCV(model, grid,\n",
    "                                           cv=5, return_train_score=False)\n",
    "grid_search.fit(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 0.0001, 'max_iter': 10000.0, 'solver': 'lbfgs'}, 0.965567688920069)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = grid_search.best_estimator_ \n",
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining the model (V 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_undersampled = LogisticRegression(C=1, penalty='l2')\n",
    "cmat, pred = RunModel(lr_undersampled, X_undersampled_train, Y_undersampled_train, X_undersampled_test, Y_undersampled_test)\n",
    "PrintStats(cmat, Y_undersampled_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=accuracy_score(Y_undersampled_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       143\n",
      "           1       0.95      0.92      0.94       153\n",
      "\n",
      "    accuracy                           0.94       296\n",
      "   macro avg       0.94      0.94      0.94       296\n",
      "weighted avg       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(Y_undersampled_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Experiment: artifact_location='mlflow-artifacts:/7067', experiment_id='7067', lifecycle_stage='active', name='exp_demo-7', tags={'projectId': '91', 'projectName': 'sync-job-test', 'workspaceId': '6'}>\n",
      "Registering model from sklearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'demo-7' already exists. Creating a new version of this model...\n",
      "2022/11/16 10:40:08 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: demo-7, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInfo(artifact_path='demo-7', flavors={'python_function': {'model_path': 'model.pkl', 'loader_module': 'mlflow.sklearn', 'python_version': '3.7.12', 'env': 'conda.yaml'}, 'sklearn': {'pickled_model': 'model.pkl', 'sklearn_version': '1.0.2', 'serialization_format': 'cloudpickle', 'code': None}}, model_uri='runs:/02f1eb24557d484ba14a8944bc9a045c/demo-7', model_uuid='9d0f243b15c44af4953346936f6d8351', run_id='02f1eb24557d484ba14a8944bc9a045c', saved_input_example_info=None, signature_dict=None, utc_time_created='2022-11-16 10:40:06.193635', mlflow_version='1.28.1.dev0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'demo-7'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run created successfully'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onetru.analytics.register_model('demo-7',lr,'scikit_learn',grid,{\"Accuracy_score\":ac})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of the Model to the original data test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining the model (V 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1, penalty='l2')\n",
    "cmat, pred = RunModel(lr, X_undersampled_train, Y_undersampled_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skplt.metrics.plot_confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     85308\n",
      "           1       0.04      0.91      0.08       135\n",
      "\n",
      "    accuracy                           0.97     85443\n",
      "   macro avg       0.52      0.94      0.53     85443\n",
      "weighted avg       1.00      0.97      0.98     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Experiment: artifact_location='mlflow-artifacts:/7067', experiment_id='7067', lifecycle_stage='active', name='exp_demo-7', tags={'projectId': '91', 'projectName': 'sync-job-test', 'workspaceId': '6'}>\n",
      "Registering model from sklearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'demo-7' already exists. Creating a new version of this model...\n",
      "2022/11/16 10:40:24 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: demo-7, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInfo(artifact_path='demo-7', flavors={'python_function': {'model_path': 'model.pkl', 'loader_module': 'mlflow.sklearn', 'python_version': '3.7.12', 'env': 'conda.yaml'}, 'sklearn': {'pickled_model': 'model.pkl', 'sklearn_version': '1.0.2', 'serialization_format': 'cloudpickle', 'code': None}}, model_uri='runs:/1fbac613065745d2a8b0ce3728e1be09/demo-7', model_uuid='8744e8f012e34816a216667996539f0e', run_id='1fbac613065745d2a8b0ce3728e1be09', saved_input_example_info=None, signature_dict=None, utc_time_created='2022-11-16 10:40:21.426518', mlflow_version='1.28.1.dev0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'demo-7'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run created successfully'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onetru.analytics.register_model('demo-7',lr,'scikit_learn',grid,{\"Accuracy_score\":ac})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric : Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1, penalty='l2')\n",
    "clf.fit(X_undersampled_train, Y_undersampled_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_pred_probability = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_probability)\n",
    "auc = metrics.roc_auc_score(y_test, pred)\n",
    "plt.plot(fpr,tpr,label=\"LogisticRegression, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
